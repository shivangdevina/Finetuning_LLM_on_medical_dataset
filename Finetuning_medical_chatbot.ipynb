{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetuning Llama 3\nWe’ll fine-tune the Llama 3 8B-Chat model using the ruslanmv/ai-medical-chatbot dataset. The dataset contains 250k dialogues between a patient and a doctor.","metadata":{}},{"cell_type":"markdown","source":"/kaggle/input/llama-3/transformers/8b-chat-hf/1\n\nGPU P100 \n\nhuggingface_token and wandb to be active\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:57:18.664666Z","iopub.execute_input":"2025-01-08T15:57:18.664883Z","iopub.status.idle":"2025-01-08T15:57:19.015716Z","shell.execute_reply.started":"2025-01-08T15:57:18.664862Z","shell.execute_reply":"2025-01-08T15:57:19.014849Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ignore the warnings","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:57:30.370701Z","iopub.execute_input":"2025-01-08T15:57:30.371029Z","iopub.status.idle":"2025-01-08T15:57:30.374707Z","shell.execute_reply.started":"2025-01-08T15:57:30.371001Z","shell.execute_reply":"2025-01-08T15:57:30.373729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:57:30.724262Z","iopub.execute_input":"2025-01-08T15:57:30.724599Z","iopub.status.idle":"2025-01-08T15:58:23.317545Z","shell.execute_reply.started":"2025-01-08T15:57:30.724571Z","shell.execute_reply":"2025-01-08T15:58:23.316598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"transformers: A library for state-of-the-art natural language processing.\n\ndatasets: A library for easily accessing and sharing datasets.\n\naccelerate: A library for optimizing and accelerating model training.\n\npeft: A library for parameter-efficient fine-tuning.\n\ntrl: A library for training language models with reinforcement learning.\n\nbitsandbytes: A library for 8-bit optimizers and quantization.\n\nwandb: A tool for experiment tracking and model management.\n","metadata":{}},{"cell_type":"code","source":"# !pip uninstall peft huggingface_hub\n# !pip install peft==0.11.0 huggingface_hub==0.23.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:58:49.843038Z","iopub.execute_input":"2025-01-08T15:58:49.843343Z","iopub.status.idle":"2025-01-08T15:58:49.847117Z","shell.execute_reply.started":"2025-01-08T15:58:49.843321Z","shell.execute_reply":"2025-01-08T15:58:49.84603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip show peft huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:50:44.772148Z","iopub.execute_input":"2025-01-08T15:50:44.772545Z","iopub.status.idle":"2025-01-08T15:50:48.329963Z","shell.execute_reply.started":"2025-01-08T15:50:44.772515Z","shell.execute_reply":"2025-01-08T15:50:48.328703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:58:54.966225Z","iopub.execute_input":"2025-01-08T15:58:54.966661Z","iopub.status.idle":"2025-01-08T15:59:09.381252Z","shell.execute_reply.started":"2025-01-08T15:58:54.966633Z","shell.execute_reply":"2025-01-08T15:59:09.380314Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code snippet is primarily setting up the necessary imports for a machine learning task involving natural language processing (NLP) using the Hugging Face Transformers library, PEFT (Parameter-Efficient Fine-Tuning), and other related tools. Here's a breakdown:\n\n\nHugging Face Transformers Imports:\n\n\n\nAutoModelForCausalLM, AutoTokenizer: For loading pre-trained language models and tokenizers.\n\nBitsAndBytesConfig, HfArgumentParser, TrainingArguments: For configuring model training and parsing arguments.\n\npipeline, logging: For creating NLP pipelines and logging.\n\n\nPEFT Imports:\n\n\n\nLoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model: For applying parameter-efficient fine-tuning techniques to models.\n\n\nOther Imports:\n\n\n\nos, torch, wandb: Standard libraries for operating system interactions, PyTorch (deep learning), and Weights & Biases (experiment tracking).\n\ndatasets: For loading datasets.\n\ntrl: Specific tools for training language models, including SFTTrainer and setup_chat_format.\n\n\n","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:59:18.190123Z","iopub.execute_input":"2025-01-08T15:59:18.190745Z","iopub.status.idle":"2025-01-08T15:59:32.753386Z","shell.execute_reply.started":"2025-01-08T15:59:18.190719Z","shell.execute_reply":"2025-01-08T15:59:32.752488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"ruslanmv/ai-medical-chatbot\"\nnew_model = \"llama-3-8b-chat-doctor\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:59:32.754585Z","iopub.execute_input":"2025-01-08T15:59:32.75486Z","iopub.status.idle":"2025-01-08T15:59:32.759175Z","shell.execute_reply.started":"2025-01-08T15:59:32.754838Z","shell.execute_reply":"2025-01-08T15:59:32.758242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:59:32.760409Z","iopub.execute_input":"2025-01-08T15:59:32.760651Z","iopub.status.idle":"2025-01-08T15:59:32.776541Z","shell.execute_reply.started":"2025-01-08T15:59:32.760602Z","shell.execute_reply":"2025-01-08T15:59:32.775905Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the model and tokenizer\n\nIn this part, we’ll load the model from Kaggle. However, due to memory constraints, we’re unable to load the full model. Therefore, we’re loading the model using 4-bit precision.\n\nOur goal in this project is to reduce memory usage and speed up the fine-tuning process.","metadata":{}},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T15:59:32.777522Z","iopub.execute_input":"2025-01-08T15:59:32.777806Z","iopub.status.idle":"2025-01-08T16:01:17.269382Z","shell.execute_reply.started":"2025-01-08T15:59:32.777777Z","shell.execute_reply":"2025-01-08T16:01:17.268726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the tokenizer and then set up a model and tokenizer for conversational AI tasks. By default, it uses the chatml template from OpenAI, which will convert the input text into a chat-like format.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:01:38.374589Z","iopub.execute_input":"2025-01-08T16:01:38.374895Z","iopub.status.idle":"2025-01-08T16:01:39.346201Z","shell.execute_reply.started":"2025-01-08T16:01:38.374872Z","shell.execute_reply":"2025-01-08T16:01:39.344907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Adding the adapter to the layer\nFine-tuning the full model will take a lot of time, so to improve the training time, we’ll attach the adapter layer with a few parameters, making the entire process faster and more memory-efficient.\n\n\n","metadata":{}},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:01:46.418415Z","iopub.execute_input":"2025-01-08T16:01:46.418754Z","iopub.status.idle":"2025-01-08T16:01:47.176548Z","shell.execute_reply.started":"2025-01-08T16:01:46.418726Z","shell.execute_reply":"2025-01-08T16:01:47.175649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the dataset\nTo load and pre-process our dataset, we:\n\n1. Load the ruslanmv/ai-medical-chatbot dataset, shuffle it, and select only the top 1000 rows. This will significantly reduce the training time.\n\n2. Format the chat template to make it conversational. Combine the patient questions and doctor responses into a \"text\" column.\n\n3. Display a sample from the text column (the “text” column has a chat-like format with special tokens).\n\n\n","metadata":{}},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:01:51.528426Z","iopub.execute_input":"2025-01-08T16:01:51.528728Z","iopub.status.idle":"2025-01-08T16:02:00.765654Z","shell.execute_reply.started":"2025-01-08T16:01:51.528706Z","shell.execute_reply":"2025-01-08T16:02:00.764928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. Split the dataset into a training and validation set.\n","metadata":{}},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:06.831654Z","iopub.execute_input":"2025-01-08T16:02:06.831997Z","iopub.status.idle":"2025-01-08T16:02:06.846572Z","shell.execute_reply.started":"2025-01-08T16:02:06.831971Z","shell.execute_reply":"2025-01-08T16:02:06.845626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Complaining and training the model\nWe are setting the model hyperparameters so that we can run it on the Kaggle. You can learn about each hyperparameter by reading the Fine-Tuning Llama 2 tutorial.\n\nWe are fine-tuning the model for one epoch and logging the metrics using the Weights and Biases.\n\n\n","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:10.899676Z","iopub.execute_input":"2025-01-08T16:02:10.900036Z","iopub.status.idle":"2025-01-08T16:02:10.935399Z","shell.execute_reply.started":"2025-01-08T16:02:10.900008Z","shell.execute_reply":"2025-01-08T16:02:10.93453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code snippet is configuring the training parameters for a machine learning model using the TrainingArguments class. Here's a breakdown of the most relevant parts:\n\n\n\noutput_dir=new_model: Specifies the directory where the trained model and other outputs will be saved.\n\nper_device_train_batch_size=1 and per_device_eval_batch_size=1: Sets the batch size for training and evaluation to 1 per device.\n\ngradient_accumulation_steps=2: Accumulates gradients over 2 steps before performing a backward pass, effectively simulating a larger batch size.\n\noptim=\"paged_adamw_32bit\": Chooses the optimizer, in this case, a 32-bit version of AdamW.\n\nnum_train_epochs=1: Sets the number of training epochs to 1.\n\nevaluation_strategy=\"steps\" and eval_steps=0.2: Specifies that evaluation should be done every 0.2 steps.\n\nlogging_steps=1 and logging_strategy=\"steps\": Logs training metrics every step.\n\nwarmup_steps=10: Sets the number of warmup steps for learning rate scheduling.\n\nlearning_rate=2e-4: Sets the learning rate to 0.0002.\n\nfp16=False and bf16=False: Disables 16-bit and bfloat16 precision training.\n\ngroup_by_length=True: Groups sequences of similar lengths together to optimize training efficiency.\n\nreport_to=\"wandb\": Specifies that training metrics should be reported to Weights and Biases (wandb) for tracking.\n","metadata":{}},{"cell_type":"code","source":"# Add a new pad_token to the tokenizer\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))  # Update model's embeddings to include the new token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:16.349463Z","iopub.execute_input":"2025-01-08T16:02:16.349796Z","iopub.status.idle":"2025-01-08T16:02:46.240545Z","shell.execute_reply.started":"2025-01-08T16:02:16.349767Z","shell.execute_reply":"2025-01-08T16:02:46.239863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing function to tokenize and truncate/pad sequences\ndef preprocess_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n# Apply preprocessing to train and test datasets\ntokenized_train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\ntokenized_test_dataset = dataset[\"test\"].map(preprocess_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:49.350992Z","iopub.execute_input":"2025-01-08T16:02:49.351293Z","iopub.status.idle":"2025-01-08T16:02:50.488844Z","shell.execute_reply.started":"2025-01-08T16:02:49.35127Z","shell.execute_reply":"2025-01-08T16:02:50.487867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We’ll now set up a supervised fine-tuning (SFT) trainer and provide\n# a train and evaluation dataset, LoRA configuration, training argument, \n# tokenizer, and model. We’re keeping the max_seq_length to 512 to avoid \n# exceeding GPU memory during training.\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:54.787139Z","iopub.execute_input":"2025-01-08T16:02:54.787451Z","iopub.status.idle":"2025-01-08T16:02:55.015103Z","shell.execute_reply.started":"2025-01-08T16:02:54.787427Z","shell.execute_reply":"2025-01-08T16:02:55.014228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:02:56.87951Z","iopub.execute_input":"2025-01-08T16:02:56.879841Z","iopub.status.idle":"2025-01-08T16:36:07.518264Z","shell.execute_reply.started":"2025-01-08T16:02:56.879812Z","shell.execute_reply":"2025-01-08T16:36:07.517538Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model evaluation\nWhen you finish the Weights & Biases session, it’ll generate the run history and summary.\n","metadata":{}},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:39:57.884614Z","iopub.execute_input":"2025-01-08T16:39:57.884931Z","iopub.status.idle":"2025-01-08T16:40:00.792586Z","shell.execute_reply.started":"2025-01-08T16:39:57.884909Z","shell.execute_reply":"2025-01-08T16:40:00.791991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have a bad scar on my forehead. How do I get rid of it?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:40:43.457154Z","iopub.execute_input":"2025-01-08T16:40:43.457474Z","iopub.status.idle":"2025-01-08T16:40:59.182837Z","shell.execute_reply.started":"2025-01-08T16:40:43.457451Z","shell.execute_reply":"2025-01-08T16:40:59.18203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the model file\nWe’ll now save the fine-tuned adapter and push it to the Hugging Face Hub. The Hub API will automatically create the repository and store the adapter file.\n\n\n","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T16:41:12.807564Z","iopub.execute_input":"2025-01-08T16:41:12.808021Z","iopub.status.idle":"2025-01-08T16:43:05.333833Z","shell.execute_reply.started":"2025-01-08T16:41:12.807983Z","shell.execute_reply":"2025-01-08T16:43:05.333056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}